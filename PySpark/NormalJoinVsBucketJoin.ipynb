{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Normal Join vs Bucket Join"
      ],
      "metadata": {
        "id": "R8o6Aj_USlj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyUv9hJDS5_s",
        "outputId": "b54348e9-9b78-4fd2-d96f-95fd4a79655f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Join"
      ],
      "metadata": {
        "id": "qZmdCNZAVQfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Normal Join Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data1 = [(1, \"John\"), (2, \"Jane\"), (3, \"Jake\"), (4, \"Jill\"), (5, \"Joe\")]\n",
        "data2 = [(1, 1000), (2, 1500), (3, 2000), (4, 2500), (5, 3000)]\n",
        "\n",
        "# Define schema\n",
        "schema1 = [\"id\", \"name\"]\n",
        "schema2 = [\"id\", \"salary\"]\n",
        "\n",
        "# Create DataFrames\n",
        "df1 = spark.createDataFrame(data1, schema1)\n",
        "df2 = spark.createDataFrame(data2, schema2)\n",
        "\n",
        "# Perform normal join with timing\n",
        "start_time = time.time()\n",
        "normal_joined_df = df1.join(df2, \"id\")\n",
        "end_time = time.time()\n",
        "\n",
        "# Show the result\n",
        "normal_joined_df.show()\n",
        "\n",
        "# Calculate and print execution time\n",
        "execution_time = end_time - start_time\n",
        "print(\"Execution Time (Normal Join): {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNp0hjc6Tn0Y",
        "outputId": "b0f2144c-fb58-4a85-b77f-0c0604411511"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+\n",
            "| id|name|salary|\n",
            "+---+----+------+\n",
            "|  1|John|  1000|\n",
            "|  2|Jane|  1500|\n",
            "|  3|Jake|  2000|\n",
            "|  4|Jill|  2500|\n",
            "|  5| Joe|  3000|\n",
            "+---+----+------+\n",
            "\n",
            "Execution Time (Normal Join): 0.14 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bucket Join"
      ],
      "metadata": {
        "id": "_g5u4h2YVTbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Bucket Join Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data1 = [(1, \"John\"), (2, \"Jane\"), (3, \"Jake\"), (4, \"Jill\"), (5, \"Joe\")]\n",
        "data2 = [(1, 1000), (2, 1500), (3, 2000), (4, 2500), (5, 3000)]\n",
        "\n",
        "# Define schema\n",
        "schema1 = [\"id\", \"name\"]\n",
        "schema2 = [\"id\", \"salary\"]\n",
        "\n",
        "# Create DataFrames\n",
        "df1 = spark.createDataFrame(data1, schema1)\n",
        "df2 = spark.createDataFrame(data2, schema2)\n",
        "\n",
        "# Save DataFrames as bucketed tables\n",
        "df1.write.bucketBy(4, \"id\").saveAsTable(\"bucketed_table1\")\n",
        "df2.write.bucketBy(4, \"id\").saveAsTable(\"bucketed_table2\")\n",
        "\n",
        "# Read the bucketed tables\n",
        "bucketed_df1 = spark.table(\"bucketed_table1\")\n",
        "bucketed_df2 = spark.table(\"bucketed_table2\")\n",
        "\n",
        "# Perform bucket join with timing\n",
        "start_time = time.time()\n",
        "bucket_joined_df = bucketed_df1.join(bucketed_df2, \"id\")\n",
        "end_time = time.time()\n",
        "\n",
        "# Show the result\n",
        "bucket_joined_df.show()\n",
        "\n",
        "# Calculate and print execution time\n",
        "execution_time = end_time - start_time\n",
        "print(\"Execution Time (Bucket Join): {:.2f} seconds\".format(execution_time))\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDjKgaLRSr37",
        "outputId": "f0f9a561-a88b-4001-9c67-a43d0100a9cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+\n",
            "| id|name|salary|\n",
            "+---+----+------+\n",
            "|  4|Jill|  2500|\n",
            "|  5| Joe|  3000|\n",
            "|  2|Jane|  1500|\n",
            "|  3|Jake|  2000|\n",
            "|  1|John|  1000|\n",
            "+---+----+------+\n",
            "\n",
            "Execution Time (Bucket Join): 0.03 seconds\n"
          ]
        }
      ]
    }
  ]
}